{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets transformers evaluate peft trl bitsandbytes accelerate\n",
    "# !pip install --upgrade -q accelerate\n",
    "# !pip install -q python-Levenshtein\n",
    "# !pip install -q langchain langchain-openai\n",
    "# !pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "\n",
    "from utils import setup_api_key\n",
    "\n",
    "setup_api_key(file_path='../../config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "            \n",
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "def get_model_id(model_type, run_name, project_name, checkpoint_id):\n",
    "    return os.path.join(model_type, \"model_output\", project_name, run_name, checkpoint_id)\n",
    "\n",
    "project_config = {\n",
    "    \"survey-json\": {\n",
    "        \"project_name\": \"survey-json-model-inst\",\n",
    "        \"train_dataset_path\": \"../datasets/survey_json_datasets_instruction_train\",\n",
    "        \"test_dataset_path\": \"../datasets/survey_json_datasets_instruction_test\",\n",
    "    },\n",
    "    \"schema\": {\n",
    "        \"project_name\": \"schema-model-inst\",\n",
    "        \"train_dataset_path\": \"../datasets/schema_datasets/schema_data_train\",\n",
    "        \"test_dataset_path\": \"../datasets/schema_datasets/schema_data_test\"\n",
    "    },\n",
    "    \"paraloq\": {\n",
    "        \"project_name\": \"paraloq-model-inst\",\n",
    "        \"train_dataset_path\": \"../datasets/paraloq/paraloq_data_train\",\n",
    "        \"test_dataset_path\": \"../datasets/paraloq/paraloq_data_test\"\n",
    "    },\n",
    "    \"nous\": {\n",
    "        \"project_name\": \"nous-model-inst\",\n",
    "        \"train_dataset_path\": \"../datasets/nous/nous_data_train\",\n",
    "        \"test_dataset_path\": \"../datasets/nous/nous_data_test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_project(project=\"schema\"):\n",
    "    test_dataset = load_from_disk(project_config[project][\"test_dataset_path\"])\n",
    "    train_dataset = load_from_disk(project_config[project][\"train_dataset_path\"])\n",
    "    return test_dataset, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import json_score, extract_json, content_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "run_list = [\"schema\", \"paraloq\", \"nous\"]\n",
    "for project in run_list:\n",
    "    print(f\"Running for project: {project}\")\n",
    "    # get_result(project=project)\n",
    "\n",
    "    # load json file\n",
    "    with open(f\"./{project}_instruction_generation.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    overall_score = 0\n",
    "    overall_key_score = 0\n",
    "    overall_value_score = 0\n",
    "    foramt_error_count = 0\n",
    "    json_array_count = 0\n",
    "    n = len(data[\"generated_responses\"])\n",
    "    y_true = [True] * n\n",
    "    y_pred = []\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            generated_response = data[\"generated_responses\"][i]\n",
    "            actual_response = data[\"actual_responses\"][i]\n",
    "            generated_json, format_error = extract_json(\n",
    "                generated_response, project=project\n",
    "            )\n",
    "            actual_json, _ = extract_json(actual_response, project=project)\n",
    "            if format_error[\"format_error\"]:\n",
    "                foramt_error_count += 1\n",
    "            scores = json_score(generated_json, actual_json)\n",
    "            if scores[\"key\"] >= 0.99 and scores[\"value\"] >= 0.8:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "            overall_score += scores[\"overall\"]\n",
    "            overall_key_score += scores[\"key\"]\n",
    "            overall_value_score += scores[\"value\"]\n",
    "            if scores[\"json_array\"]:\n",
    "                json_array_count += 1\n",
    "                \n",
    "            # get content score\n",
    "            content_scores = content_score(generated_response, actual_response)\n",
    "            print(f\"Content score: {content_scores}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    avg_score = overall_score / n\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    print(f\"Average score: {avg_score}\")\n",
    "    print(f\"Average key score: {overall_key_score / n}\")\n",
    "    print(f\"Average value score: {overall_value_score / n}\")\n",
    "    print(f\"Count of format errors: {foramt_error_count}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_model = ChatOpenAI(temperature=0)\n",
    "# class Metric(BaseModel):\n",
    "#     setup: str = Field(description=\"question to set up a joke\")\n",
    "#     punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# # Set up a parser + inject instructions into the prompt template.\n",
    "# parser = JsonOutputParser(pydantic_object=Metric)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "#     input_variables=[\"query\"],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "\n",
    "# chain = prompt | chain_model | parser\n",
    "\n",
    "# metric_query = f\"tell me a joke\"\n",
    "# chain.invoke({\"query\": metric_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
